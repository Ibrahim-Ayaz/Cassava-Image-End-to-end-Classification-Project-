{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibrahim-Ayaz/Cassava-Image-End-to-end-Classification-Project-/blob/main/cassava_image_classification_project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uRx5yn0kbdx"
      },
      "source": [
        "### Cassava Image Classification End-to-End Project with TensorFlow\n",
        "\n",
        "Cassava consists of leaf images for the cassava plant depicting healthy and four (4) disease conditions; Cassava Mosaic Disease (CMD), Cassava Bacterial Blight (CBB), Cassava Greem Mite (CGM) and Cassava Brown Streak Disease (CBSD). Dataset consists of a total of 9430 labelled images. The 9430 labelled images are split into a training set (5656), a test set(1885) and a validation set (1889). The number of images per class are unbalanced with the two disease classes CMD and CBSD having 72% of the images.\n",
        "\n",
        "For more, you can refer to the following link: https://arxiv.org/abs/1806.02987\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tndfrpZ3kbdy"
      },
      "source": [
        "## Check for GPU access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTAkBSTTkbdz"
      },
      "outputs": [],
      "source": [
        "# Confirm access to a GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOVHpfK0kbdz"
      },
      "source": [
        "## Downloading `helper_functions.py` script\n",
        "\n",
        "The script contains useful helper functions we need when calculating model metrics, as well as plotting model results such as ROC/loss curves and confusion matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkc3Rsb7kbdz"
      },
      "outputs": [],
      "source": [
        "# Download helper function script\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK0t04zEkbdz"
      },
      "outputs": [],
      "source": [
        "# Import the necessary dependencies (functions) from the script\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys, make_confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTu9DPX7kbdz"
      },
      "source": [
        "## Downloading the Cassava dataset from TensorFlow Datasets\n",
        "\n",
        "We're going to be downloading and loading the dataset from TensorFlow datasets: https://www.tensorflow.org/datasets/catalog/cassava"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MIdf6wLkbd0"
      },
      "outputs": [],
      "source": [
        "# Check if our required dataset is in TensorFlow datasets\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets = tfds.list_builders()\n",
        "print('cassava' in datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fl2H41ekbd0"
      },
      "outputs": [],
      "source": [
        "# Download/load the dataset\n",
        "# Note: 'cassava' typically includes a train split and may include a validation/test split depending on TFDS version\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "builder = tfds.builder('cassava')\n",
        "available_splits = set(builder.info.splits.keys())\n",
        "print('Available splits:', available_splits)\n",
        "\n",
        "val_split = 'validation' if 'validation' in available_splits else ('test' if 'test' in available_splits else None)\n",
        "if val_split is None:\n",
        "    # Fallback: if no val/test split exists, we'll carve a small validation set out of train later.\n",
        "    (train_data,), ds_info = tfds.load(\n",
        "        name = 'cassava',\n",
        "        split=['train'],\n",
        "        as_supervised = True,\n",
        "        with_info = True\n",
        "    )\n",
        "    test_data = None\n",
        "else:\n",
        "    (train_data, test_data), ds_info = tfds.load(\n",
        "        name = 'cassava',\n",
        "        split = ['train', val_split],\n",
        "        as_supervised = True,\n",
        "        with_info = True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt63eLAsBxj8"
      },
      "outputs": [],
      "source": [
        "# If TFDS didn't provide a validation/test split, create one from the train set\n",
        "import tensorflow as tf\n",
        "\n",
        "if test_data is None:\n",
        "    # Shuffle once deterministically and split 90/10\n",
        "    train_count = ds_info.splits['train'].num_examples\n",
        "    val_count = int(0.1 * train_count)\n",
        "    train_data = train_data.shuffle(buffer_size = min(10_000, train_count), seed = 42, reshuffle_each_iteration = False)\n",
        "    test_data = train_data.take(val_count)\n",
        "    train_data = train_data.skip(val_count)\n",
        "    print(f'Created validation split from train: train = {train_count - val_count}, val = {val_count}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtkMti1dkbd0"
      },
      "source": [
        "## Visualising samples from the dataset\n",
        "\n",
        "**Note:** When doing ML/DL experiments, it's important to familiarise yourself with the samples within the dataset; that way, you'll have an intuition of how your experiments should be laid out, and what types of prprocessing and model selection will be required. Most importantly, view **random** samples since randomness makes it powerful when visualing each feature/characteristic of an example.\n",
        "\n",
        "The explorer data's motto: visualise, visualise, visualise!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM7-9wFWkbd0"
      },
      "outputs": [],
      "source": [
        "# Show samples from the dataset\n",
        "tfds.show_examples(train_data, ds_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYDBTgcBkbd0"
      },
      "source": [
        "## Create efficient data preprocessing functions & pipelines for our modelling experiments\n",
        "\n",
        "We've now viewed some random images of each label, let's start building some efficient data preprocessing functions and pipelines to make our experiments run computationally fast and efficient as much as possible: https://www.tensorflow.org/guide/data_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAV_NsMRkbd0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define data augmentation layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip('horizontal'),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "], name = 'data_augmentation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NmJD5xdkbd0"
      },
      "outputs": [],
      "source": [
        "# Create preprocessing function for our data\n",
        "def preprocess_image(image, label, image_size = 224):\n",
        "  image = tf.image.resize(image, [image_size, image_size]) # Reshape target image to 224x224\n",
        "  image = tf.cast(image, dtype = tf.float32)\n",
        "  image = data_augmentation(image) # Pass image through data augmentation layer\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-YaqG8ykbd0"
      },
      "outputs": [],
      "source": [
        "# Compare between original and transformed image\n",
        "\n",
        "# Get a sample and label from dataset\n",
        "for image, label in train_data.take(1):\n",
        "  transformed_image = preprocess_image(image = image, label = label)\n",
        "\n",
        "# Get the transformed image and label from the tuple (the tuple comes from the preprocessing function above)\n",
        "preprocessed_image, transformed_label = transformed_image\n",
        "\n",
        "# Print out image before and after prepcrocessing & shapes\n",
        "print(f'Original image before preprocessing: \\n{image[:0]}')\n",
        "print(f'Original image shape: \\n{image.shape}')\n",
        "print(f'Original image datatype: {image.dtype}')\n",
        "print(f'Preprocessed image: \\n{preprocessed_image[:0]}')\n",
        "print(f'Preprocessed image shape: \\n{preprocessed_image.shape}')\n",
        "print(f'Preprocessed image datatype: \\n{preprocessed_image.dtype}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLIprEo2kbd0"
      },
      "source": [
        "## Converting our datasets to prefected datasets and batched for faster computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU9Cm66nkbd1"
      },
      "outputs": [],
      "source": [
        "# Map preprocessing function, shuffle and parallelise it to training data\n",
        "train_data = train_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "train_data = train_data.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Validation/test pipeline\n",
        "test_data = test_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0JqaAYGkbd1"
      },
      "source": [
        "## Create a ModelCheckpoint callback to save our models' weights during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XoFi0T4kbd1"
      },
      "outputs": [],
      "source": [
        "# Create model checkpoint callback path\n",
        "checkpoint_path = 'cassava_model_checkpoint.weights.h5'\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path, save_weights_only = True, monitor = 'val_accuracy', save_best_only = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VALNwXbkbd1"
      },
      "source": [
        "## Turning on mixed precision training for faster training time\n",
        "\n",
        "When running an experiments with a large amount of data, it is important to turn on mixed precision training so that the training time is decreased whilst still maintaining the precision metric score: https://www.tensorflow.org/guide/mixed_precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGss-7v5kbd1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16') # Set global data policy for mixed precision\n",
        "mixed_precision.global_policy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlpwB2Nkkbd1"
      },
      "source": [
        "## Modelling Experiments\n",
        "\n",
        "Now we're going to be conducting a series of modelling experiments to find out which model outperforms all other models, so that we can deploy it to a mobile app or to a web application for testing custom images.\n",
        "\n",
        "And when doing experiments as such, it's always good to start with a baseline and use it as your benchmark to see if other models can beat it or not.\n",
        "\n",
        "And always experiment as much as you can so you can discover the pattern in each modelling experiment, i.e. changed in model's metrics or observing what happens when you change hyperparameters; the experiment practitioner's motto: experiment, experiment, experiment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4BXC-Njkbd1"
      },
      "source": [
        "### Model 0 (baseline): ResNetV2101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5LRnwXNkbd1"
      },
      "outputs": [],
      "source": [
        "# Setup data inputs\n",
        "input_size = (224, 224, 3)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "# Create feature extractor model with Functional API\n",
        "base_model = tf.keras.applications.ResNet101V2(include_top = False, weights = 'imagenet', input_shape = input_size)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape = (224, 224, 3), name = 'input_layer')\n",
        "x = base_model(inputs, training = False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(256, activation = 'relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation = 'softmax', name = 'output_layer')(x)\n",
        "model_0 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Conmpile model\n",
        "model_0.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "# Get model summary\n",
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42wO9iUQkbd1"
      },
      "outputs": [],
      "source": [
        "# Fit\n",
        "initial_epochs = 10\n",
        "history_0 = model_0.fit(train_data, epochs = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_0'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okua2DfWkbd1"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves for baseline\n",
        "plot_loss_curves(history = history_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ54k5NXkbd2"
      },
      "outputs": [],
      "source": [
        "# Evaluate baseline on test data\n",
        "model_0.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UI1JLJpkbd2"
      },
      "outputs": [],
      "source": [
        "# Make predictions with baseline\n",
        "model_0_preds = model_0.predict(test_data)\n",
        "model_0_preds[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jt9M8O0kbd2"
      },
      "outputs": [],
      "source": [
        "# Convert baseline preds to labels\n",
        "labels = model_0_preds.argmax(axis = 1)\n",
        "labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGv2Wrqqkbd2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get true labels from the test dataset\n",
        "true_labels = []\n",
        "for images, labels_batch in test_data.unbatch():\n",
        "    true_labels.append(labels_batch.numpy())\n",
        "\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Plot model 0's confusion matrix\n",
        "make_confusion_matrix(y_true = true_labels, y_pred = labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M97JEihCkbd2"
      },
      "source": [
        "### Fine-tuning baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRJb0_iikbd2"
      },
      "outputs": [],
      "source": [
        "# Unfreeze last 20 layers of feature-extractor model\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Freeze all BatchNormalization layers within the last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "  if isinstance(layer, layers.BatchNormalization):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile\n",
        "model_0.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "# Fit fine-tuned model\n",
        "history_0_fine_tune = model_0.fit(train_data, epochs = total_epochs, initial_epoch = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_0_fine-tuned'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrmjjhXRkbd2"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves for fine-tuned baseline\n",
        "plot_loss_curves(history = history_0_fine_tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hQE43VHkbd2"
      },
      "outputs": [],
      "source": [
        "# Compare original and fine-tuned model historys for baseline\n",
        "compare_historys(original_history = history_0, new_history = history_0_fine_tune, initial_epochs = initial_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyJxv1YGkbd2"
      },
      "source": [
        "### Model 1: EfficientNetV2B0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAdBe10Dkbd2"
      },
      "outputs": [],
      "source": [
        "# Setup data inputs\n",
        "input_size = (224, 224, 3)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "# Create feature extractor model with Functional API\n",
        "base_model = tf.keras.applications.EfficientNetV2B0(include_top = False, weights = 'imagenet', input_shape = input_size)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape = (224, 224, 3), name = 'input_layer')\n",
        "x = base_model(inputs, training = False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(256, activation = 'relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation = 'softmax', name = 'output_layer')(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Conmpile model\n",
        "model_1.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "# Get model summary\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z2yJVJckbd2"
      },
      "outputs": [],
      "source": [
        "# Fit\n",
        "initial_epochs = 10\n",
        "history_1 = model_1.fit(train_data, epochs = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_1'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj4Pd0oLkbd2"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves for model 1\n",
        "plot_loss_curves(history = history_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UpoL6Y8kbd2"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on test data\n",
        "model_1.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnFqaBA8kbd3"
      },
      "outputs": [],
      "source": [
        "# Make predictions with model 1\n",
        "model_1_preds = model_1.predict(test_data)\n",
        "model_1_preds[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxKfYeelkbd3"
      },
      "outputs": [],
      "source": [
        "# Convert model 1 preds to labels\n",
        "labels = model_1_preds.argmax(axis = 1)\n",
        "labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9G1ko6Akbd3"
      },
      "outputs": [],
      "source": [
        "# Plot model 1's confusion matrix\n",
        "make_confusion_matrix(y_true = true_labels, y_pred = labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcst3U-lkbd3"
      },
      "source": [
        "### Fine-tuning model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEh0vf53kbd3"
      },
      "outputs": [],
      "source": [
        "# Unfreeze last 20 layers of feature-extractor model\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Freeze all BatchNormalization layers within the last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "  if isinstance(layer, layers.BatchNormalization):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile\n",
        "model_1.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "# Fit fine-tuned model\n",
        "history_1_fine_tune = model_1.fit(train_data, epochs = total_epochs, initial_epoch = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_1_fine-tuned'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJBJOZvUkbd3"
      },
      "outputs": [],
      "source": [
        "# Plot fine-tuned model 1 loss curves\n",
        "plot_loss_curves(history = history_1_fine_tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hODyoiZkbd4"
      },
      "outputs": [],
      "source": [
        "# Compare original and fine-tuned model 1 historys\n",
        "compare_historys(original_history = history_1, new_history = history_1_fine_tune, initial_epochs = initial_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvk4mhzekbd4"
      },
      "source": [
        "### Model 2: EfficientNetV2B1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iwiHgz1kbd4"
      },
      "outputs": [],
      "source": [
        "# Setup data inputs\n",
        "input_size = (224, 224, 3)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "# Create feature extractor model with Functional API\n",
        "base_model = tf.keras.applications.EfficientNetV2B1(include_top = False, weights = 'imagenet', input_shape = input_size)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape = (224, 224, 3), name = 'input_layer')\n",
        "x = base_model(inputs, training = False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(256, activation = 'relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation = 'softmax', name = 'output_layer')(x)\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Conmpile model\n",
        "model_2.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "# Get model summary\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y4pCNCCkbd4"
      },
      "outputs": [],
      "source": [
        "# Fit\n",
        "initial_epochs = 10\n",
        "history_2 = model_2.fit(train_data, epochs = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_2'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KefsxTNqkbd4"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves for model 2\n",
        "plot_loss_curves(history = history_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B08qNMRckbd4"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on test data\n",
        "model_2.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-aoK3H3kbd4"
      },
      "outputs": [],
      "source": [
        "# Make predictions with model 2\n",
        "model_2_preds = model_2.predict(test_data)\n",
        "model_2_preds[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDmyJO0Tkbd4"
      },
      "outputs": [],
      "source": [
        "# Convert model 2 preds to labels\n",
        "labels = model_2_preds.argmax(axis = 1)\n",
        "labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD9_iccnkbd4"
      },
      "outputs": [],
      "source": [
        "# Plot model 2's confusion matrix\n",
        "make_confusion_matrix(y_true = true_labels, y_pred = labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCDuVzWKkbd5"
      },
      "source": [
        "### Fine-tuning model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HakVagY5kbd5"
      },
      "outputs": [],
      "source": [
        "# Unfreeze last 20 layers of feature-extractor model\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Freeze all BatchNormalization layers within the last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "  if isinstance(layer, layers.BatchNormalization):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile\n",
        "model_2.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "# Fit fine-tuned model\n",
        "history_2_fine_tune = model_2.fit(train_data, epochs = total_epochs, initial_epoch = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_2_fine-tuned'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDOZcY6skbd5"
      },
      "outputs": [],
      "source": [
        "# Plot fine-tuned model 1 loss curves\n",
        "plot_loss_curves(history = history_2_fine_tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYP6EMLJkbd5"
      },
      "outputs": [],
      "source": [
        "# Compare orignal and fine-tuned historys for model 2\n",
        "compare_historys(original_history = history_2, new_history = history_2_fine_tune, initial_epochs = initial_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3wyBXcxkbd5"
      },
      "source": [
        "### Model 3: ResNet152V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzqGh8vDkbd5"
      },
      "outputs": [],
      "source": [
        "# Setup data inputs\n",
        "input_size = (224, 224, 3)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "# Create feature extractor model with Functional API\n",
        "base_model = tf.keras.applications.ResNet152V2(include_top = False, weights = 'imagenet', input_shape = input_size)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape = (224, 224, 3), name = 'input_layer')\n",
        "x = base_model(inputs, training = False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(256, activation = 'relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation = 'softmax', name = 'output_layer')(x)\n",
        "model_3 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Conmpile model\n",
        "model_3.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "# Get model summary\n",
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vu1Y_q7kbd5"
      },
      "outputs": [],
      "source": [
        "# Fit\n",
        "initial_epochs = 10\n",
        "history_3 = model_3.fit(train_data, epochs = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_3'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y_FV-Vkkbd5"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves for model 3\n",
        "plot_loss_curves(history = history_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ4cLzbekbd5"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "model_3.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deMeA7lekbd6"
      },
      "outputs": [],
      "source": [
        "# Make predictions with model 3\n",
        "model_3_preds = model_3.predict(test_data)\n",
        "model_3_preds[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5mHFdnGkbd6"
      },
      "outputs": [],
      "source": [
        "# Convert predictions to labels for model 3\n",
        "labels = model_3_preds.argmax(axis = 1)\n",
        "labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pManlZfOkbd6"
      },
      "outputs": [],
      "source": [
        "# Plot model 3's confusion matrix\n",
        "make_confusion_matrix(y_true = true_labels, y_pred = labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvrH-dh2kbd6"
      },
      "source": [
        "### Fine-tuning model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR83VJW4kbd6"
      },
      "outputs": [],
      "source": [
        "# Unfreeze last 20 layers of feature-extractor model\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Freeze all BatchNormalization layers within the last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "  if isinstance(layer, layers.BatchNormalization):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile\n",
        "model_3.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "# Fit fine-tuned model\n",
        "history_3_fine_tune = model_3.fit(train_data, epochs = total_epochs, initial_epoch = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_3_fine-tuned'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNtGrPaPkbd6"
      },
      "outputs": [],
      "source": [
        "# Plot fine-tuned model 3's loss curves\n",
        "plot_loss_curves(history = history_3_fine_tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCjtH99Lkbd6"
      },
      "outputs": [],
      "source": [
        "# Compare orginal and fine-tuned historys for model 3\n",
        "compare_historys(original_history = history_3, new_history = history_3_fine_tune, initial_epochs = initial_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUCWcjbekbd6"
      },
      "source": [
        "### Model 4: Mobilenet_v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytPmygOOkbd6"
      },
      "outputs": [],
      "source": [
        "# Setup data inputs\n",
        "input_size = (224, 224, 3)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "# Create feature extractor model with Functional API\n",
        "base_model = tf.keras.applications.MobileNetV3Large(include_top = False, weights = 'imagenet', input_shape = input_size)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape = (224, 224, 3), name = 'input_layer')\n",
        "x = base_model(inputs, training = False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(256, activation = 'relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation = 'softmax', name = 'output_layer')(x)\n",
        "model_4 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Conmpile model\n",
        "model_4.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "# Get model summary\n",
        "model_4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlaenBnFkbd6"
      },
      "outputs": [],
      "source": [
        "# Fit\n",
        "initial_epochs = 10\n",
        "history_4 = model_4.fit(train_data, epochs = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_4'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNrilWI1kbd6"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves\n",
        "plot_loss_curves(history = history_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSBje1oDkbd7"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "model_4.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg35uM48kbd7"
      },
      "outputs": [],
      "source": [
        "# Make predictions with model\n",
        "model_4_preds = model_4.predict(test_data)\n",
        "model_4_preds[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8wfPeK2kbd7"
      },
      "outputs": [],
      "source": [
        "# Convert predictions to labels\n",
        "labels = model_4_preds.argmax(axis = 1)\n",
        "labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbXa2qMxkbd7"
      },
      "outputs": [],
      "source": [
        "# Plot model 4's confusion matrix\n",
        "make_confusion_matrix(y_true = true_labels, y_pred = labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytita5bvkbd7"
      },
      "source": [
        "### Fine-tuning model 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgYunGSPkbd7"
      },
      "outputs": [],
      "source": [
        "# Unfreeze last 20 layers of feature-extractor model\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Freeze all BatchNormalization layers within the last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "  if isinstance(layer, layers.BatchNormalization):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile\n",
        "model_4.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "# Fit fine-tuned model\n",
        "history_4_fine_tune = model_4.fit(train_data, epochs = total_epochs, initial_epoch = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_4_fine-tuned'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQNwLIdZkbd7"
      },
      "outputs": [],
      "source": [
        "# Plot fine-tuned model's loss curves\n",
        "plot_loss_curves(history = history_4_fine_tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8iM20Ftkbd7"
      },
      "outputs": [],
      "source": [
        "# Compare original and fine-tuned model historys\n",
        "compare_historys(original_history = history_4, new_history = history_4_fine_tune, initial_epochs = initial_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgBypZX8kbd7"
      },
      "source": [
        "### Model 5: ConvNeXtBase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waeqLZaIkbd7"
      },
      "outputs": [],
      "source": [
        "# Setup data inputs\n",
        "input_size = (224, 224, 3)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "# Create feature extractor model with Functional API\n",
        "base_model = tf.keras.applications.ConvNeXtBase(include_top = False, weights = 'imagenet', input_shape = input_size)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape = (224, 224, 3), name = 'input_layer')\n",
        "x = base_model(inputs, training = False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(256, activation = 'relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation = 'softmax', name = 'output_layer')(x)\n",
        "model_5 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Conmpile model\n",
        "model_5.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "# Get model summary\n",
        "model_5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZztcU-1kbd7"
      },
      "outputs": [],
      "source": [
        "# Fit\n",
        "initial_epochs = 10\n",
        "history_5 = model_5.fit(train_data, epochs = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_5'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9bVEtNikbd8"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves\n",
        "plot_loss_curves(history = history_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBB2B1ICkbd8"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "model_5.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHBP8ebwkbd8"
      },
      "outputs": [],
      "source": [
        "# Make predictions with model\n",
        "model_5_preds = model_5.predict(test_data)\n",
        "model_5_preds[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycTqiAkfkbd8"
      },
      "outputs": [],
      "source": [
        "# Convert preds to labels\n",
        "labels = model_5_preds.argmax(axis = 1)\n",
        "labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_3QgWj2kbd8"
      },
      "outputs": [],
      "source": [
        "# Plot model 5's confusion matrix\n",
        "make_confusion_matrix(y_true = true_labels, y_pred = labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sL8ySnMkbd8"
      },
      "source": [
        "### Fine-tuning model 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "140AGywQkbd8"
      },
      "outputs": [],
      "source": [
        "# Unfreeze last 20 layers of feature-extractor model\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Freeze all BatchNormalization layers within the last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "  if isinstance(layer, layers.BatchNormalization):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile\n",
        "model_5.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "# Fit fine-tuned model\n",
        "history_5_fine_tune = model_5.fit(train_data, epochs = total_epochs, initial_epoch = initial_epochs, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15 * len(test_data)), callbacks = [create_tensorboard_callback(dir_name = './tensorflow_logs', experiment_name = 'cassava_all_modelling_experiments/model_5_fine-tuned'), model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0HeC2Bckbd8"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves for fine-tuned model\n",
        "plot_loss_curves(history = history_5_fine_tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7xUBrh7kbd8"
      },
      "outputs": [],
      "source": [
        "# Compare orginal and fine-tuned model historys\n",
        "compare_historys(original_history = history_5, new_history = history_5_fine_tune, initial_epochs = initial_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2GZVOzdkbd8"
      },
      "source": [
        "## Uploading all of our modelling experiments to TensorBoard\n",
        "\n",
        "We're now going to be uploading all of our modelling experiments to TensorBoard to view each metric for all of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAMtxkqJkbd8"
      },
      "outputs": [],
      "source": [
        "# Upload models' results to TensorBoard\n",
        "!pip install -q tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=/content/tensorflow_logs\n",
        "%reload_ext tensorboard"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}